{"config":{"lang":["en"],"separator":"[\\s\\-]+"},"docs":[{"title":"Getting Started with ContainerLab","text":"","location":""},{"title":"Introduction","text":"<p>ContainerLab is an open source project that provides a quick and easy method to emulate network toplogies. The examples on this site will make use of Arista's cEOS-lab, but there is wide ranging vendor support for both containerized (and non-containerized) Network Operating Systems.</p> <p>This guide will focus on how to quickly get started with ContainerLab.</p>","location":"#introduction"},{"title":"Requirements","text":"<p>Getting up and running with ContainerLab requires the following:</p>  <ul> <li>Linux host (this guide uses Ubuntu 20.04)</li> <li>Sufficient memory (1) and compute (2) to run desired topologies</li> <li>Docker</li> <li>ContainerLab</li> <li>Containerized Network Operating System of choice (3)</li> </ul>  <ol> <li> <p> Containerized Network Operating Systems are much more resource efficient than their tradtionally virtualized conterparts. For example, as of 4.30.0F, each cEOS-lab node consumes ~850MB of memory when up and running. vEOS-lab requires 4GB.</p> </li> <li> <p> Topologies are most CPU intensive at boot. There is a <code>startup-delay</code> parameter that can be used to help manage this when a topology is too large to simultaneously boot all nodes. A list of node parameters can be found here. This is covered in more detail in the Creating a Topology section.</p> </li> <li> <p> We will be using cEOS-Lab for this guide. However, we can use other Network Operating Systems to create large multi-vendor topologies as needed.</p> </li> </ol>","location":"#requirements"},{"title":"Installation","text":"","location":"#installation"},{"title":"Install Docker","text":"<p>The first step is to install docker on the Linux host. These instructions cover, step by step, how to perform this isntallation on Ubuntu 20.04</p>","location":"#install-docker"},{"title":"Install ContainerLab","text":"<p>This one-line command (taken from this guide) will install ContainerLab on the Linux host:</p> <pre><code># download and install the latest release (may require sudo)\nbash -c \"$(curl -sL https://get.containerlab.dev)\"\n</code></pre> <p>We can validate that ContainerLab was installed by using the <code>sudo clab version</code> command:</p> <pre><code>mitch@mitchlab2:~$ sudo clab version\n\n                           _                   _       _\n                 _        (_)                 | |     | |\n ____ ___  ____ | |_  ____ _ ____   ____  ____| | ____| | _\n/ ___) _ \\|  _ \\|  _)/ _  | |  _ \\ / _  )/ ___) |/ _  | || \\\n( (__| |_|| | | | |_( ( | | | | | ( (/ /| |   | ( ( | | |_) )\n\\____)___/|_| |_|\\___)_||_|_|_| |_|\\____)_|   |_|\\_||_|____/\n\n    version: 0.41.2\n     commit: fe51f41f\n       date: 2023-05-18T14:57:44Z\n     source: https://github.com/srl-labs/containerlab\n rel. notes: https://containerlab.dev/rn/0.41/#0412\n</code></pre>","location":"#install-containerlab"},{"title":"Download Containerized NOS","text":"<p>We will be using Arista's cEOS-lab in this guide. This is a free download that can be found under the cEOS-lab directory here (1)</p> <ol> <li> When downloading cEOS-lab, choose one of the images ending in <code>.tar.xz</code></li> </ol>","location":"#download-containerized-nos"},{"title":"Import Containerized NOS","text":"<p>Before defining, and booting, a topology; The containerized NOS will need to imported into Docker. This can be done via the following command:</p> <pre><code>sudo docker import [path to cEOS-lab .tar.xz file] ceos:[release ID]\n</code></pre> <p>As a more complete example, here is a command that imports cEOS64-lab-4.30.0F.tar.xz into docker as ceos:4.30.0F</p> <pre><code>sudo docker import cEOS64-lab-4.30.0F.tar.xz ceos:4.30.0F #(1)!\n</code></pre> <ol> <li> This command was run from the directory where the cEOS64-lab-4.30.0F.tar.xz image was located. This is not a requirement, but makes the example command easier to read.</li> </ol> <p>We can validate that the image was imported by using the <code>sudo docker images</code> command:</p> <pre><code>mitch@mitchlab2:~$ sudo docker images\nREPOSITORY            TAG       IMAGE ID       CREATED         SIZE\nceos                  trunk     1d73d2ef5663   7 days ago      2.37GB\nceos                  4.30.0F   0ca9c82f8c3b   2 weeks ago     2.47GB\nmitchv85/devhost      latest    973bcbccca1d   3 weeks ago     1.83GB\nnetreplica/graphite   latest    96300cf0339b   11 months ago   206MB\n</code></pre>","location":"#import-containerized-nos"},{"title":"Creating a Topology","text":"<p>At this point, we have Docker host running on Ubuntu 20.04 with ContainerLab installed. We have also downloaded, and imported, cEOS-lab into Docker. </p> <p>Now, let's define a topology file that ContainerLab can use to create, and deploy, a topology</p> <p>For reference, here is the topology we will be creating:</p> <p></p> <p>ContainerLab makes use of YAML files to create, and deploy, topologies. For this topology, we'll create a file named <code>lab.yml</code> (1). This file, and it's components, are shown below for reference:</p> <ol> <li> The topology file can be named whatever we'd like. We are just calling it <code>lab.yml</code>for simplicity.</li> </ol>  Note <p>This guide does not make use of all possible parameters in the topology file. For a complete list of parameters/options that can be defined in a ContainerLab topology file, please refer to the ContainerLab Documentation</p>  <pre><code>---\n# -------------------------------------------------------------\n# L3LS EVPN Demo Topology\n# 2 Spines &amp; 2 Leaf Pairs with MLAG\n# -------------------------------------------------------------\n\nname: evpn-demo #(1)!\nprefix: \"\" #(2)!\n\nmgmt: #(3)!\n  network: mgmt\n  ipv4-subnet: 172.100.100.0/24\n\ntopology:\n\n  defaults:\n    env:\n      INTFTYPE: et #(4)!\n\n  kinds: #(5)!\n    ceos:\n      image: ceos:4.30.0F #(6)!\n    linux:\n      image: mitchv85/devhost #(7)!\n\n  nodes: #(8)!\n\n#########################\n# SPINES                #\n#########################\n\n    SPINE1: #(9)!\n      kind: ceos #(10)!\n      mgmt-ipv4: 172.100.100.101 #(11)!\n      startup-config: startup-configs/SPINE1.cfg #(12)!\n      ports: #(13)!\n        - '22001:22' #(14)!\n        - '8001:80' #(15)!\n        - '44301:443' #(16)!\n\n    SPINE2:\n      kind: ceos\n      mgmt-ipv4: 172.100.100.102\n      startup-config: startup-configs/SPINE2.cfg\n      ports:\n        - '22002:22'\n        - '8002:80'\n        - '44302:443'\n\n#########################\n# LEAF                  #\n#########################\n\n    LEAF1:\n      kind: ceos\n      mgmt-ipv4: 172.100.100.103\n      startup-config: startup-configs/LEAF1.cfg\n      ports:\n        - '22003:22'\n        - '8003:80'\n        - '44303:443'\n\n    LEAF2:\n      kind: ceos\n      mgmt-ipv4: 172.100.100.104\n      startup-config: startup-configs/LEAF2.cfg\n      ports:\n        - '22004:22'\n        - '8004:80'\n        - '44304:443'\n\n    LEAF3:\n      kind: ceos\n      mgmt-ipv4: 172.100.100.105\n      startup-config: startup-configs/LEAF3.cfg\n      ports:\n        - '22005:22'\n        - '8005:80'\n        - '44305:443'\n\n    LEAF4:\n      kind: ceos\n      mgmt-ipv4: 172.100.100.106\n      startup-config: startup-configs/LEAF4.cfg\n      ports:\n        - '22006:22'\n        - '8006:80'\n        - '44306:443'\n\n###########################################\n# HOSTS                                   #\n###########################################\n\n    HostA:\n      kind: linux\n      mgmt-ipv4: 172.100.100.201\n      ports:\n        - '22201:22'\n      exec:\n        - bash /usr/local/bin/hostnetconfig.sh -b -i 10.10.10.101/24 -g 10.10.10.1 #(17)!\n\n    HostB:\n      kind: linux\n      mgmt-ipv4: 172.100.100.202\n      ports:\n        - '22202:22'\n      exec:\n        - bash /usr/local/bin/hostnetconfig.sh -b -i 10.30.30.101/24 -g 10.30.30.1\n\n    HostC:\n      kind: linux\n      mgmt-ipv4: 172.100.100.203\n      ports:\n        - '22203:22'\n      exec:\n        - bash /usr/local/bin/hostnetconfig.sh -b -i 10.10.10.102/24 -g 10.10.10.1\n\n    HostD:\n      kind: linux\n      mgmt-ipv4: 172.100.100.204\n      ports:\n        - '22204:22'\n      exec:\n        - bash /usr/local/bin/hostnetconfig.sh -b -i 10.20.20.101/24 -g 10.20.20.1\n\n  links: #(18)!\n\n####################\n# SPINE1 to LEAF   #\n####################\n    - endpoints: [\"SPINE1:et1\", \"LEAF1:et1\"] #(19)!\n    - endpoints: [\"SPINE1:et2\", \"LEAF2:et1\"]\n    - endpoints: [\"SPINE1:et3\", \"LEAF3:et1\"]\n    - endpoints: [\"SPINE1:et4\", \"LEAF4:et1\"]\n\n####################\n# SPINE2 to LEAF   #\n####################\n    - endpoints: [\"SPINE2:et1\", \"LEAF1:et2\"]\n    - endpoints: [\"SPINE2:et2\", \"LEAF2:et2\"]\n    - endpoints: [\"SPINE2:et3\", \"LEAF3:et2\"]\n    - endpoints: [\"SPINE2:et4\", \"LEAF4:et2\"]\n\n##################\n# LEAF1 to LEAF2 #\n##################\n    - endpoints: [\"LEAF1:et5\", \"LEAF2:et5\"]\n    - endpoints: [\"LEAF1:et6\", \"LEAF2:et6\"]\n\n####################\n# LEAF3 to LEAF4   #\n####################\n    - endpoints: [\"LEAF3:et5\", \"LEAF4:et5\"]\n    - endpoints: [\"LEAF3:et6\", \"LEAF4:et6\"]\n\n####################\n# HOSTA            #\n####################\n    - endpoints: [\"HostA:eth1\", \"LEAF1:et3\"]\n    - endpoints: [\"HostA:eth2\", \"LEAF2:et3\"]\n\n####################\n# HOSTB            #\n####################\n    - endpoints: [\"HostB:eth1\", \"LEAF1:et4\"]\n    - endpoints: [\"HostB:eth2\", \"LEAF2:et4\"]\n\n####################\n# HOSTC            #\n####################\n    - endpoints: [\"HostC:eth1\", \"LEAF3:et3\"]\n    - endpoints: [\"HostC:eth2\", \"LEAF4:et3\"]\n\n####################\n# HOSTD            #\n####################\n    - endpoints: [\"HostD:eth1\", \"LEAF3:et4\"]\n    - endpoints: [\"HostD:eth2\", \"LEAF4:et4\"]\n</code></pre> <ol> <li> The name of the topology</li> <li> (Optional) Setting the \"prefix\" of each lab node name to null. By default, the name of the lab is prepended to every node (container) that is created in the topology.</li> <li> (Optional) Each node will have an IP assigned to it from this block. This is the equivalent of an \"Out of Band Management\" network in the cLab world; This network only exists within the cLab host (by default), and is used by the nodes for communication to/from the world outside of the Docker host.</li> <li> (Optional) This setting is unique to cEOS-lab. It is required in order for the OSPF and IS-IS routing protocols to function properly on cEOS-lab nodes.</li> <li> This is where we define the different kinds of containerized nodes we will be using in our topology.</li> <li> Anywhere we define a node as <code>ceos</code>, it will run our imported 4.30.0F cEOS-lab image</li> <li> Anywhere we define a node as <code>linux</code>, it will run the publicly available mitchv85/devhost image.</li> <li> Here we start defining the nodes that will exist in our topology.</li> <li> Defines the <code>SPINE1</code> node</li> <li> Sets <code>SPINE1</code> node as running <code>ceos</code> kind of container. Which we set to ceos:4.30.0F</li> <li> (Optional) Statically define the management address that we'd like assigned to the <code>SPINE1</code> node.</li> <li> (Optional) Path to a defined startup-config that the <code>SPINE1</code> node should use when the topology is deployed</li> <li> (Optional) Define a list of ports on the Docker Host that we'd like forwarded to the <code>SPINE1</code> node</li> <li> (Optional) If an inbound connection to TCP port 22001 is received on the Docker host, it will forward this to TCP port 22 (SSH) on the <code>SPINE1</code> node</li> <li> (Optional) If an inbound connection to TCP port 8001 is received on the Docker host, it will forward this to TCP port 80 (HTTP) on the <code>SPINE1</code> node</li> <li> (Optional) If an inbound connection to TCP port 44301 is received on the Docker host, it will forward this to TCP port 443 (HTTP) on the <code>SPINE1</code> node</li> <li> (Optional) Unique to the mitchv85/devhost image. Instructs cLab to run the <code>hostnetconfig.sh</code> shell script to configure IP addressing and/or LACP after deploying the node. More informaiton here</li> <li> The <code>links</code> section is where we will define how nodes are to be connected together</li> <li> Define a link between <code>SPINE1, Ethernet1</code> and <code>LEAF1, Ethernet1</code>. This can be customized for modular interfaces as well. For example, <code>et1_1_1</code> would evaluate to <code>Ethernet1/1/1</code> in cEOS-lab.</li> </ol>","location":"#creating-a-topology"},{"title":"Deploying a Topology","text":"<p>Next, let's tell ContainerLab to deploy our topology.</p> <p>First, we'll check to see if any other topologies are running on the ContainerLab host by using the <code>clab inspect</code> command.</p> <pre><code>mitch@mitchlab2:~$ sudo clab inspect -a\nINFO[0000] no containers found\n</code></pre> <p>The output above indicates that no other topologies are running on the ContainerLab host.</p> <p>ContainerLab supports multiple topologies running in parallel on the same host. But, in this example, we want to make sure our topology is the only one running. Perhaps the ContainerLab host we're running only has enough resources to run our topology.</p> <p>Next, we'll use the <code>clab deploy</code> command to deploy our topology.</p> <pre><code>mitch@mitchlab2:~$ sudo clab deploy -t lab.yaml #(1)!\n</code></pre> <ol> <li>In this example, <code>lab.yaml</code> exists in the current working directory. A path to the topology file could be specified here as well.</li> </ol> <p>The output of this command, resulting in a successfully running topology, is below for reference:</p> <pre><code>INFO[0000] Containerlab v0.41.2 started\nINFO[0000] Parsing &amp; checking topology file: lab.yaml\nINFO[0000] Creating lab directory: /home/mitch/avd-demo/clab-evpn-demo\nINFO[0000] Creating docker network: Name=\"mgmt\", IPv4Subnet=\"172.100.100.0/24\", IPv6Subnet=\"\", MTU=\"1500\"\nINFO[0000] Creating container: \"HostB\"\nINFO[0000] Creating container: \"HostA\"\nINFO[0000] Creating container: \"HostC\"\nINFO[0000] Creating container: \"HostD\"\nINFO[0000] Creating container: \"SPINE2\"\nINFO[0000] Creating container: \"LEAF2\"\nINFO[0000] Creating container: \"LEAF4\"\nINFO[0000] Creating container: \"LEAF1\"\nINFO[0000] Creating container: \"SPINE1\"\nINFO[0000] Creating container: \"LEAF3\"\nINFO[0001] Creating virtual wire: SPINE2:et3 &lt;--&gt; LEAF3:et2\nINFO[0001] Creating virtual wire: SPINE2:et4 &lt;--&gt; LEAF4:et2\nINFO[0001] Creating virtual wire: LEAF1:et6 &lt;--&gt; LEAF2:et6\nINFO[0001] Creating virtual wire: LEAF3:et5 &lt;--&gt; LEAF4:et5\nINFO[0001] Creating virtual wire: SPINE2:et1 &lt;--&gt; LEAF1:et2\nINFO[0001] Creating virtual wire: SPINE1:et2 &lt;--&gt; LEAF2:et1\nINFO[0001] Creating virtual wire: HostC:eth2 &lt;--&gt; LEAF4:et3\nINFO[0001] Creating virtual wire: HostC:eth1 &lt;--&gt; LEAF3:et3\nINFO[0001] Creating virtual wire: SPINE1:et3 &lt;--&gt; LEAF3:et1\nINFO[0001] Creating virtual wire: SPINE2:et2 &lt;--&gt; LEAF2:et2\nINFO[0001] Creating virtual wire: HostD:eth1 &lt;--&gt; LEAF3:et4\nINFO[0001] Creating virtual wire: SPINE1:et4 &lt;--&gt; LEAF4:et1\nINFO[0001] Creating virtual wire: SPINE1:et1 &lt;--&gt; LEAF1:et1\nINFO[0001] Creating virtual wire: HostD:eth2 &lt;--&gt; LEAF4:et4\nINFO[0001] Creating virtual wire: LEAF1:et5 &lt;--&gt; LEAF2:et5\nINFO[0001] Creating virtual wire: LEAF3:et6 &lt;--&gt; LEAF4:et6\nINFO[0004] Creating virtual wire: HostA:eth1 &lt;--&gt; LEAF1:et3\nINFO[0004] Creating virtual wire: HostB:eth2 &lt;--&gt; LEAF2:et4\nINFO[0004] Creating virtual wire: HostB:eth1 &lt;--&gt; LEAF1:et4\nINFO[0004] Creating virtual wire: HostA:eth2 &lt;--&gt; LEAF2:et3\nINFO[0005] Running postdeploy actions for Arista cEOS 'SPINE2' node\nINFO[0005] Running postdeploy actions for Arista cEOS 'LEAF4' node\nINFO[0005] Running postdeploy actions for Arista cEOS 'SPINE1' node\nINFO[0005] Running postdeploy actions for Arista cEOS 'LEAF1' node\nINFO[0005] Running postdeploy actions for Arista cEOS 'LEAF3' node\nINFO[0005] Running postdeploy actions for Arista cEOS 'LEAF2' node\nINFO[0048] Adding containerlab host entries to /etc/hosts file\nINFO[0048] Executed command \"bash /usr/local/bin/hostnetconfig.sh -b -i 10.10.10.101/24 -g 10.10.10.1\" on the node \"HostA\". stdout:\nINFO[0048] Executed command \"bash /usr/local/bin/hostnetconfig.sh -b -i 10.10.10.102/24 -g 10.10.10.1\" on the node \"HostC\". stdout:\nINFO[0048] Executed command \"bash /usr/local/bin/hostnetconfig.sh -b -i 10.20.20.101/24 -g 10.20.20.1\" on the node \"HostD\". stdout:\nINFO[0048] Executed command \"bash /usr/local/bin/hostnetconfig.sh -b -i 10.30.30.101/24 -g 10.30.30.1\" on the node \"HostB\". stdout:\n+----+--------+--------------+------------------+-------+---------+--------------------+--------------+\n| #  |  Name  | Container ID |      Image       | Kind  |  State  |    IPv4 Address    | IPv6 Address |\n+----+--------+--------------+------------------+-------+---------+--------------------+--------------+\n|  1 | HostA  | 9adcf1907483 | mitchv85/devhost | linux | running | 172.100.100.201/24 | N/A          |\n|  2 | HostB  | afc129768461 | mitchv85/devhost | linux | running | 172.100.100.202/24 | N/A          |\n|  3 | HostC  | dd028144e1e4 | mitchv85/devhost | linux | running | 172.100.100.203/24 | N/A          |\n|  4 | HostD  | aca2670837e0 | mitchv85/devhost | linux | running | 172.100.100.204/24 | N/A          |\n|  5 | LEAF1  | 1a7608f9dde2 | ceos:4.30.0F     | ceos  | running | 172.100.100.103/24 | N/A          |\n|  6 | LEAF2  | d1da14cad69f | ceos:4.30.0F     | ceos  | running | 172.100.100.104/24 | N/A          |\n|  7 | LEAF3  | 88dbd2141afb | ceos:4.30.0F     | ceos  | running | 172.100.100.105/24 | N/A          |\n|  8 | LEAF4  | 87767a4684db | ceos:4.30.0F     | ceos  | running | 172.100.100.106/24 | N/A          |\n|  9 | SPINE1 | 2ab18e4d33ad | ceos:4.30.0F     | ceos  | running | 172.100.100.101/24 | N/A          |\n| 10 | SPINE2 | 04d7a820b0ae | ceos:4.30.0F     | ceos  | running | 172.100.100.102/24 | N/A          |\n+----+--------+--------------+------------------+-------+---------+--------------------+--------------+\n</code></pre> <p>At this point, the topology is running and we can start interacting with it</p>","location":"#deploying-a-topology"},{"title":"Interacting with a Topology","text":"<p>Connectivity to nodes in a given topology can be established in multiple ways. This guide will focus on two methods:</p> <ol> <li>SSH directly to the nodes from a remote workstation</li> <li>Connecting to the console of the nodes via the ContainerLab host</li> </ol>","location":"#interacting-with-a-topology"},{"title":"SSH","text":"<p>In order to SSH to the nodes from a remote workstation, a few items need to be in place first</p> <ol> <li>The <code>Management0</code> interface on the node has an IP address in <code>mgmt.ipv4-subnet</code> network defined in the topology file</li> <li>A default route pointing to a next-hop of the first usable address in the <code>mgmt.ipv4-subnet</code> network.</li> </ol> <p>By default, the first usable address of the network defined in <code>mgmt.ipv4-subnet</code> is owned by the ContainerLab host, and is the out of band management \"gateway\" for the nodes in the topology.</p> <p>For example, here is a snippet of the <code>SPINE1</code> node's startup-configuration</p> <pre><code>vrf instance MGMT #(1)!\n!\ninterface Management0\n   vrf MGMT #(2)!\n   ip address 172.100.100.101/24 #(3)!\n!\nip route vrf MGMT 0.0.0.0/0 172.100.100.1 #(4)!\n!\nmanagement ssh\n   vrf MGMT\n      no shutdown #(5)!\n!\nusername admin privilege 15 role network-admin secret admin #(6)!\n!\n</code></pre> <ol> <li> Define a VRF for Management.</li> <li>Place the <code>Management0</code> interface into the MGMT VRF.</li> <li>Statically define the IP address of the <code>Management0</code> interface.</li> <li>Set a default route for the MGMT VRF. The next-hop is the first usable address in the network specified by <code>mgmt.ipv4-subnet</code> in the topology file.</li> <li>Enable SSH on VRF MGMT on the cEOS-lab node.</li> <li>Define a user that can be used when remotely accessing the node via SSH.</li> </ol>  Tip <p>Even though we've manually specified this information in the startup-config, ContainerLab can handle some of this automatically. More informaiton can be found here</p>  <p>With this configuration in place, we can now SSH to the node from a remote SSH session.</p> <p>For this example we'll SSH to the <code>SPINE1</code> node, definied in our lab.yaml topology file, which is running on a ContainerLab host named <code>mitchlab2.packetanglers.com</code></p> <pre><code>PS C:\\Users\\Mitch&gt; ssh -p 22001 admin@mitchlab2.packetanglers.com\n(admin@mitchlab2.packetanglers.com) Password:\nLast login: Tue May 30 18:51:48 2023 from 24.29.218.185\nSPINE1# exit\nConnection to mitchlab2.packetanglers.com closed.\nPS C:\\Users\\Mitch&gt;\n</code></pre> <p>In the above output, we SSH'd to TCP port <code>22001</code>, which is what we definied in the port forwarding for the <code>SPINE1</code> node in our topology file</p> <pre><code>SPINE1:\n    kind: ceos\n    mgmt-ipv4: 172.100.100.101\n    startup-config: startup-configs/SPINE1.cfg\n    ports:\n    - '22001:22'\n    - '8001:80'\n    - '44301:443'\n</code></pre> <p>A high level illustration of the SSH connectivity method, used to access <code>SPINE1</code> from a remote host, is shown below</p> <p></p>","location":"#ssh"},{"title":"Console via cLab Host","text":"<p>Sometimes, usually for troubleshooting or getting some initial configuration on the node, there is a need to connect to the node's console directly from the ContainerLab host.</p> <p>This can be done via the <code>docker exec</code> command, with an example for cEOS-lab shown below:</p> <pre><code>mitch@mitchlab2:~$ sudo docker exec -it SPINE1 Cli\nSPINE1&gt;en\nSPINE1#\n</code></pre>  Tip <p>The name of the node to connect to, in the example above <code>SPINE1</code>, can be found via the <code>sudo clab inspect</code> command.</p>","location":"#console-via-clab-host"},{"title":"Stopping a Topology","text":"<p>Stopping a topology can be accomplished via the <code>clab destroy</code> command.</p> <pre><code>mitch@mitchlab2:~$ sudo clab destroy -t lab.yaml -c\n</code></pre>  Note <p>The <code>-c</code> flag is used to specify \"Cleanup\". The full list of flags associated with the <code>destroy</code> command, and their respective use cases, can be found here</p>  <p>Once this command is entered, the topology will be stopped, and the containers associated with the nodes will be removed.</p> <p>The output of a successful <code>destroy</code> command is shown below:</p> <pre><code>INFO[0000] Parsing &amp; checking topology file: lab.yaml\nINFO[0000] Destroying lab: evpn-demo\nINFO[0000] Removed container: HostC\nINFO[0000] Removed container: HostA\nINFO[0000] Removed container: HostD\nINFO[0001] Removed container: HostB\nINFO[0001] Removed container: SPINE1\nINFO[0001] Removed container: SPINE2\nINFO[0002] Removed container: LEAF4\nINFO[0002] Removed container: LEAF2\nINFO[0002] Removed container: LEAF3\nINFO[0002] Removed container: LEAF1\nINFO[0002] Removing containerlab host entries from /etc/hosts file\n</code></pre> <p>At this point, if our topology was the only one running on the ContainerLab host, the output of <code>sudo clab inspect -a</code> should return no active topologies</p> <pre><code>mitch@mitchlab2:~$ sudo clab inspect -a\nINFO[0000] no containers found\n</code></pre> <p>Additionally, the ouptut of the <code>sudo docker ps -a</code> command should show no containers</p> <pre><code>mitch@mitchlab2:~$ sudo docker ps -a\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n</code></pre>","location":"#stopping-a-topology"},{"title":"Forcing the removal of nodes","text":"<p>In the event that an error is encountered when destroying a topology, and running or stopped containers are still listed in the output of <code>sudo docker ps -a</code>, the <code>sudo docker system prune</code> command can be used to force the removal of containers and any unused images.</p> <pre><code>mitch@mitchlab2:~$ sudo docker system prune\nWARNING! This will remove:\n  - all stopped containers\n  - all networks not used by at least one container\n  - all dangling images\n  - all dangling build cache\n\nAre you sure you want to continue? [y/N] y\nTotal reclaimed space: 0B\n</code></pre>","location":"#forcing-the-removal-of-nodes"}]}